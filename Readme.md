# Sign Language Detection

This project implements a sign language detection model designed to recognize and classify hand gestures in real-time. Using a combination of TensorFlow for deep learning, MediaPipe for efficient hand tracking, and OpenCV for video processing, the model facilitates communication by interpreting sign language gestures into meaningful outputs. It provides a valuable tool for bridging communication gaps and fostering accessibility for the hearing-impaired community.

## Features

- Real-time action detection and classification.
- Utilizes TensorFlow and Keras for model training and evaluation.
- Employs OpenCV and MediaPipe for video processing and feature extraction.

## Requirements

The project requires the following dependencies:

```bash
numpy
cv2 (OpenCV)
mediapipe
tensorflow
matplotlib
scikit-learn
```

# Installation
1. Clone the Repositery:
```
git clone https://github.com/chaudharijay/Sign-Language-Detection.git
```

# 2. Install dependencies:
```pip install numpy opencv-python mediapipe tensorflow matplotlib scikit-learn```

# 3. Run the "Action Detection.ipynb" to first train your actions.
Actions are already recorded for following phrases - Hi, Iloveyou, Thank you.


# 3. Download or prepare your dataset.
Usage:
1. Run the notebook or script to preprocess data and train the model.
2. Test the action detection using live video feed or pre-recorded videos.
3. Analyze the model's performance with accuracy scores and confusion matrices.
